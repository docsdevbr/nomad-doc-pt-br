---
# Copyright (c) 2024 HashiCorp, Inc.
# Nomad and the Nomad logo are trademarks of HashiCorp.

# Documentation licensed under the Business Source License, Version 1.1.
# The original work was translated from English into Brazilian Portuguese.
# https://github.com/hashicorp/nomad/blob/main/LICENSE

source_url: https://github.com/hashicorp/nomad/blob/main/website/content/docs/architecture/cpu.mdx
revision: 53b083b8c5e09c8f12ae6a9db9fe6f88e75c2d12
status: ready

layout: docs
page_title: Como o Nomad utiliza a CPU
description: |-
  Aprenda como o Nomad descobre e utiliza recursos de CPU em nós para alocar e
  executar cargas de trabalho.
  O Nomad Enterprise pode usar o agendamento de acesso não uniforme à memória
  (NUMA), otimizado para a topologia NUMA de um nó cliente.
---

# Como o Nomad utiliza CPUs

Esta página fornece informações conceituais sobre como o Nomad descobre e
utiliza recursos de CPU em nós para alocar e executar cargas de trabalho.

Cada nó Nomad possui uma Unidade Central de Processamento (CPU) que fornece o
poder computacional necessário para executar os processos do sistema
operacional.
O Nomad utiliza a CPU para executar tasks definidas pelo submissor de jobs do
Nomad.
Para que o Nomad saiba quais nós têm capacidade suficiente para executar uma
determinada task, cada nó no cluster é identificado por impressão digital para
coletar informações sobre as características de desempenho de sua CPU.
As duas métricas associadas a cada nó Nomad em relação ao desempenho da CPU são
sua largura de banda (quanto ele pode _computar_) e o número de núcleos.

CPUs modernas podem conter tipos de núcleo heterogêneos.
A Apple lançou a CPU M1 em 2020, que contém os tipos de _desempenho_ (núcleo P) e
_eficiência_ (núcleo E).
Cada tipo de núcleo opera em uma frequência base diferente.
A Intel introduziu uma topologia semelhante em seus chips Raptor Lake em 2022.
Ao identificar as características de uma CPU, o Nomad é capaz de levar em
consideração essas topologias avançadas de CPU.

[![Núcleos PE](/img/nomad-pe-cores.png)](/img/nomad-pe-cores.png)

## Calculando os recursos da CPU

A largura de banda total da CPU de um nó Nomad é a soma do produto entre a
frequência de cada tipo de núcleo e o número total de núcleos desse tipo na
CPU.

```
largura_de_banda = (nucleos_p * frequencia_p) + (nucleos_e * frequencia_e)
```

O número total de núcleos é calculado somando o número de núcleos P e o número
de núcleos E.

```
nucleos = nucleos_p + nucleos_e
```

O Nomad não distingue entre núcleos de CPU lógicos e físicos.
Uma das diferenças que definem os tipos de núcleo P e E é que os núcleos E não
suportam hyperthreading, enquanto os núcleos P suportam.
Assim, um único núcleo P físico é apresentado como 2 núcleos lógicos, e um único
núcleo E é apresentado como 1 núcleo lógico.

O exemplo abaixo é de um nó Nomad com uma CPU Intel i9-13900.
Ele é composto por tipos de núcleo mistos, com uma frequência base do núcleo P
de 2 GHz e uma frequência base do núcleo E de 1,5 GHz.

Essas características são refletidas nos atributos de nó
`cpu.frequency.performance` e `cpu.frequency.efficiency`, respectivamente.

```text
cpu.arch                        = amd64
cpu.frequency.efficiency        = 1500
cpu.frequency.performance       = 2000
cpu.modelname                   = 13th Gen Intel(R) Core(TM) i9-13900
cpu.numcores                    = 32
cpu.numcores.efficiency         = 16
cpu.numcores.performance        = 16
cpu.reservablecores             = 32
cpu.totalcompute                = 56000
cpu.usablecompute               = 56000
```

## Reservando recursos de CPU

Nos atributos de nó identificados, `cpu.totalcompute` indica a quantidade total
de largura de banda da CPU que o processador é capaz de fornecer.
Em alguns casos, pode ser benéfico reservar uma certa quantidade de recursos de
CPU de um nó para uso pelo sistema operacional e outros processos não Nomad.
Isso pode ser feito na configuração do cliente.

A quantidade de CPU reservada pode ser especificada em largura de banda via
`cpu`:

```hcl
client {
  reserved {
    cpu = 3000 # mhz
  }
}
```

Ou como um conjunto específico de `cores` nos quais é proibido o agendamento
de tasks do Nomad.
Este recurso está disponível apenas em sistemas Linux.

```hcl
client {
  reserved {
    cores = "0-3"
  }
}
```

Quando a CPU é limitada por uma das configurações acima, o atributo
`cpu.usablecompute` do nó indica a quantidade total de largura de banda da CPU
disponível para o agendamento de tasks do Nomad.

## Alocando recursos de CPU

Ao agendar jobs, uma task deve especificar quanto recurso de CPU deve ser
alocado em seu nome.
Isso pode ser feito em termos de largura de banda em MHz com o atributo `cpu`.
No Linux com cgroups v1, o Nomad mapeia esse valor em MHz diretamente para
[cpu.share][].
No Linux com cgroups v2, o Nomad converte o valor em MHz para um [cpu.weight][]
proporcionalmente igual ao `cpu.share` do cgroups v1.

```hcl
task {
  resources {
    cpu = 2000 # mhz
  }
}
```

Observe que o mecanismo de isolamento em torno dos recursos da CPU depende de
cada driver de task e de sua configuração.
O comportamento padrão é que o Nomad garanta que uma task tenha acesso a _pelo
menos_ a mesma quantidade de largura de banda alocada da CPU.
Nesse caso, se um nó tiver capacidade de CPU ociosa, uma task poderá usar
recursos adicionais da CPU.
Alguns drivers de task permitem limitar uma task para usar apenas a quantidade
de largura de banda alocada a ela, descrita na seção
[Limites Rígidos da CPU](#limites-rígidos-da-cpu) abaixo.

### Compartilhamentos/pesos relativos da CPU no Linux

Os cgroups do Linux são hierárquicos e os valores `cpu.share`/`cpu.weight`
refletem os pesos relativos dentro de uma determinada subárvore.
O Nomad cria sua própria subárvore de cgroups(`nomad.slice`) na inicialização, e
todos os valores `cpu.share`/`cpu.weight` que o Nomad grava são relativos entre
os processos dentro desse slice.
A subárvore `nomad.slice` é relativa a outra subárvore no host.
Por exemplo, um host executando systemd pode ter os seguintes slices:

```
/sys/fs/cgroup
├── nomad.slice
│   ├── reserve.slice
│   └── share.slice
│       ├── 912dcc05-61e1-53cb-5489-a976a1231960.task.scope
│       ├── 247e706a-6df8-4123-89b3-1bcf2846b503.task.scope
│       └── 586c0c58-3d50-4730-b4ad-022076d3c6a4.task.scope
├── system.slice
│   ├── journald.service
│   └── (various system services, etc.)
└── user-1000.slice
    ├── session-1.scope
    └── (various user services, etc.)
```

Se a task `912dcc05` tiver `resources.cpu = 1024` e as tasks `247e706a` e
`586c0c58` tiverem `resources.cpu = 512`, então `912dcc05` receberá 50% dos
recursos de CPU disponíveis para `nomad.slice` e as tasks `247e706a` e
`586c0c58` receberão 25% cada.
(`reserve.slice` e `share.slice` são passagens para compartilhamentos de CPU
aqui.)
Mas, juntas, elas receberão 33% dos recursos totais de CPU do host, a menos que
`nomad.slice`, `system.slice` ou `user-1000.slice` tenham algo diferente dos
1024 compartilhamentos padrão.
O valor 1024 só é significativo dentro do contexto do slice do Nomad.

### Alocando núcleos

Em sistemas Linux, o Nomad suporta a reserva de núcleos inteiros de CPU
especificamente para uma task.
Nenhuma task poderá ser executada em um núcleo de CPU reservado para outra task.

```hcl
task {
  resources {
    cores = 4
  }
}
```

Recomendamos o uso de `resources.cores` para tasks que exigem alto desempenho da
CPU para dar a essas tasks acesso exclusivo à largura de banda da CPU.
Tarefas sidecar nas mesmas allocations podem usar `resources.cpu` para obter uma
parte proporcional da CPU restante no nó.

O Nomad Enterprise suporta agendamento com reconhecimento de NUMA, o que permite
às pessoas operadoras controlar com mais precisão quais núcleos de CPU podem ser
reservados para tasks.

### Limites rígidos da CPU

Alguns drivers de task suportam a opção de configuração `cpu_hard_limit`.
Se habilitada, esta opção impede que as tasks ultrapassem o limite da CPU, mesmo
quando há capacidade ociosa no nó.
A compensação é consistência versus utilização.
Uma task com poucos recursos de CPU pode operar normalmente até que outra task
seja colocada no nó, causando uma redução na largura de banda da CPU disponível,
o que pode causar interrupção para a task subprovisionada.

### Variáveis de ambiente da CPU

Para ajudar as tasks a entender os recursos disponíveis, o Nomad define as
seguintes variáveis de ambiente em seu ambiente de execução.

- `NOMAD_CPU_LIMIT` - A quantidade de largura de banda da CPU alocada em nome da
  task;
- `NOMAD_CPU_CORES` - O conjunto de núcleos na notação [cpuset][] reservados
  para a task.
  Este valor só é definido se `resources.cores` estiver configurado.

```sh
NOMAD_CPU_CORES=3-5
NOMAD_CPU_LIMIT=9000
```

## NUMA

Clientes Nomad são comumente provisionados em hardware real em um ambiente
on-premise ou na nuvem em grandes instâncias `.metal`.
Em ambos os casos, é provável que o servidor subjacente seja projetado em torno
de uma [topologia de acesso não uniforme à memória (NUMA)][numa_wiki].
Servidores que contêm múltiplos soquetes de CPU ou múltiplos bancos de RAM por
soquete de CPU são caracterizados pelos tempos de acesso não uniformes
envolvidos no acesso à memória do sistema.

[![NUMA](/img/nomad-numa.png)](/img/nomad-numa.png)

A máquina de exemplo simplificada acima tem a seguinte topologia:

- 2 soquetes físicos de CPU
- 4 bancos de memória do sistema, 2 por soquete
- 8 núcleos físicos de CPU (4 por soquete)
- 2 núcleos lógicos de CPU por núcleo físico
- 4 dispositivos PCI, 1 por banco de memória

### Otimizando o desempenho

Processos do sistema operacional levam mais tempo para acessar a memória através
de uma fronteira NUMA.

Usando o exemplo acima, se uma task for agendada no núcleo `core 0`, o acesso à
memória em `mem 1` pode levar 20% mais tempo do que o acesso à memória em
`mem 0`, e o acesso à memória em `mem 2` pode levar 300% mais tempo.

As diferenças extremas se devem a várias limitações físicas de hardware.
Um núcleo acessando a memória em seu próprio nó NUMA é o ideal.
Programas que realizam uma alta taxa de leitura ou gravação de/para a memória do
sistema terão seu desempenho substancialmente prejudicado por não otimizarem sua
localização espacial em relação à topologia NUMA do sistema.

### Tabelas SLIT

Máquinas modernas definirão tabelas de Informações de Distância de Localidade do
Sistema (SLIT) em seu firmware.
Essas tabelas são compreendidas e referenciáveis pelo kernel Linux.
Há duas informações principais fornecidas pelas tabelas SLIT:

- Quais núcleos de CPU pertencem a quais nós NUMA;
- A penalidade incorrida para acessar cada nó NUMA a partir de um núcleo em cada
  outro nó NUMA.

O comando `lscpu` pode ser usado para descrever a associatividade do núcleo em
uma máquina.
Por exemplo, em uma instância EC2 `r6a.metal`:

```shell-session
$ lscpu | grep NUMA
NUMA node(s):           4
NUMA node0 CPU(s):      0-23,96-119
NUMA node1 CPU(s):      24-47,120-143
NUMA node2 CPU(s):      48-71,144-167
NUMA node3 CPU(s):      72-95,168-191
```

E as degradações de desempenho associadas estão disponíveis via `numactl`:

```shell-session
$ numactl -H
available: 4 nodes (0-3)
...
node distances:
node   0   1   2   3
  0:  10  12  32  32
  1:  12  10  32  32
  2:  32  32  10  12
  3:  32  32  12  10
```

Esses valores de distância dos nós da tabela SLIT são apresentados como
proporções relativas aproximadas.
O valor 10 representa uma situação ideal em que um acesso à memória ocorre a
partir de uma CPU que faz parte do mesmo nó NUMA.
Um valor 20 indicaria uma degradação de desempenho de 200%, 30, 300%, e assim
por diante.

### Atributos do nó

Os clientes Nomad identificarão a topologia NUMA da máquina e exportarão a
associatividade do núcleo como atributos do nó.
Esses dados podem fornecer à pessoa operadora do Nomad uma melhor compreensão de
quando pode ser útil usar o agendamento com reconhecimento de NUMA para
determinadas cargas de trabalho.

```
numa.node.count       = 4
numa.node0.cores      = 0-23,96-119
numa.node1.cores      = 24-47,120-143
numa.node2.cores      = 48-71,144-167
numa.node3.cores      = 72-95,168-191
```

## Agendamento com reconhecimento de NUMA <EnterpriseAlert inline />

O Nomad Enterprise é capaz de agendar tasks de forma otimizada para a topologia
NUMA de um nó cliente.
O Nomad é capaz de correlacionar núcleos de CPU com nós de memória e atribuir
tasks para execução em núcleos de CPU específicos, a fim de minimizar quaisquer
padrões de acesso entre nós de memória.
Além disso, o Nomad é capaz de correlacionar dispositivos a nós de memória e
permitir que o agendamento com reconhecimento de NUMA leve em consideração a
associatividade dos dispositivos ao tomar decisões de agendamento.

Uma task pode especificar um bloco `numa` indicando sua preferência de
otimização NUMA.
Este exemplo aloca uma GPU `1080ti` e garante que ela esteja no mesmo nó NUMA
que os 4 núcleos de CPU reservados para a task.

```hcl
task {
  resources {
    cores = 4
    memory = 2048

    device "nvidia/gpu/1080ti" {
      count = 1
    }

    numa {
      affinity = "require"
      devices = [
        "nvidia/gpu/1080ti"
      ]
	  }
  }
}
```

### Opções de `affinity`

Este campo é obrigatório.
Há três opções de afinidade suportadas: `none`, `prefer` e `require`, cada uma
com suas próprias vantagens e desvantagens.

#### Opção `none`

No modo `none`, o agendador Nomad aproveita a apatia de tasks sem preferência de
afinidade NUMA para ajudar a reduzir a fragmentação do núcleo dentro dos nós
NUMA.
Ele faz isso agrupando a requisição de núcleo dessas tasks nos nós NUMA com o
menor número de núcleos não utilizados disponíveis.

O modo `none` é o modo padrão se o bloco `numa` não for especificado.

```hcl
resources {
  cores = 4
  numa {
    affinity = "none"
  }
}
```

#### Opção `prefer`

No modo `prefer`, o agendador Nomad usa a topologia de hardware de um nó para
calcular uma seleção otimizada de núcleos disponíveis, mas não limita esses
núcleos a virem de um único nó NUMA.

```hcl
resources {
  cores = 4
  numa {
    affinity = "prefer"
  }
}
```

#### Opção `require`

No modo `require`, o agendador Nomad usa a topologia de cada cliente em
potencial para encontrar um conjunto de núcleos de CPU disponíveis que pertençam
ao mesmo nó NUMA.
Se nenhum conjunto de núcleos for encontrado, esse nó será marcado como esgotado
para o recurso `numa-cores`.

```hcl
resources {
  cores = 4
  numa {
    affinity = "require"
  }
}
```

### Opções de `devices`

`devices` é uma lista opcional de dispositivos que devem ser alocados no nó NUMA
junto com núcleos de CPU alocados.

O diagrama a seguir mostra como um conjunto de dispositivos pode ser
correlacionado à CPU e à memória.

[![Como um conjunto de dispositivos pode ser correlacionado à CPU e à memória](/img/nomad-devices-correlate-cpu-memory.png)](/img/nomad-devices-correlate-cpu-memory.png)

Este exemplo declara três dispositivos e configura dois no bloco `numa`.

```hcl
task {
  resources {
    cores = 8
    memory = 16384

    device "nvidia/gpu/H100" {
      count = 2
    }
    device "intel/net/XXVDA2" {
      count = 1
    }
    device "xilinx/fpga/X7" {
      count = 1
    }

    numa {
	    affinity = "require"
	    devices = [
  	    "nvidia/gpu/H100",
  	    "intel/net/XXVDA2"
	    ]
    }
  }
}
```

## Identificação da CPU virtual

Ao ser executado em um host virtualizado, como o Amazon EC2, o Nomad utiliza a
ferramenta `dmidecode` para detectar dados de desempenho da CPU.
Algumas distribuições Linux exigirão a instalação manual do pacote `dmidecode`.

[cpuset]: https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/cpusets.html
[cpu.share]: https://www.redhat.com/sysadmin/cgroups-part-two
[cpu.weight]: https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#weights
[numa_wiki]: https://pt.wikipedia.org/wiki/Acesso_n%C3%A3o_uniforme_a_mem%C3%B3ria
